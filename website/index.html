<!-- HTML template used from https://codepen.io/bramus/pen/ExaEqMJ -->
<!DOCTYPE html>
<html>
<head>
	<title>Final Project</title>
    <link rel="icon" type="image/png" href="assets/favicon.png">

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <!-- Include custom styling -->
	<link rel="stylesheet" type="text/css" href="style.css">
    
    <!-- Include js script for smooth navigation in webpage -->
    <script src="js/script.js"></script>
    
    <!-- Include fonts for a better aesthetic feel -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&family=Quicksand:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@1,300&family=Montserrat:wght@400;600;700&family=Quicksand:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@1,300&family=Montserrat:wght@400;600;700&family=Quicksand:wght@300;400;500;600;700&family=Raleway&display=swap" rel="stylesheet">
</head>

<body>
	<main>
        <div>
            <h1>Final Project - Generative Models for Illumination Recovery in Low Light</h1>
            <p><em>apraveen, lrickett, shrutina</em></p>
            <!-- Introduction -->
            <section id="introduction">
                <h2>
                    Project Description
                </h2>
                <section>
                    <p>
                        Images taken in low light or near darkness suffer from limited visibility, loss of detail, and noise. Common post-processing methods 
                        such as global brightness and exposure scaling often produce unrealistic, oversaturated images as a result and are typically unable to produce a realistic illuminated image from a low light one. 
                        In this project, we aim to explore and utilize multiple generative model architectures to recover the contents of these low light images. Recovering these details has multiple applications from making images more aesthetically pleasing to enhancing images for robotics, such as autonomous driving in the dark. 
                    </p>
                </section>
            </section>

            <!-- Enlighten-GAN -->
            <section id="enlighten-gan">
                <h2>Method 1: Enlighten GAN</h2>
                <p>
                    The first approach we explore is <a href="https://github.com/VITA-Group/EnlightenGAN">EnlightenGAN</a>, generative model that is able to enhance low-light images. This model, shown in the following figure,
                    uses unpaired images for training, improving upon previous methods that required paired low/high lighting supervision for a given image. The model also uses an inverted grayscale version of the input image as an attention map, 
                    which helps the model focus illumination efforts on the darkest portion of the image. The output of the UNet generator is the illumination component, which is then added to the original 
                    image to get the final result. EnlightenGAN then uses a combination of global and local discriminators that tell whether the image is illuminated or low-light.  
                    We use this model as a baseline for comparison with other methods, as well as try to improve upon the results of this model.
                    <figure class="normal-image-container" style="display: inline-block; width: 700px; max-width: 700px;">
                        <img src="assets/enlightengan/model.png" style="height: 250px; max-width: 700px; width: 700px;">
                        <figcaption>EnlightenGAN Model Architecture</figcaption>
                    </figure>
                
                </p>
                <section id="results-one" id="subsection">
                    <h3> Experiments / Results</h3>
                    
                    <section>
                        <h4>1. Baseline Experiment</h4>
                        <p>For a baseline comparison of this model, we downloaded pretrained weights and ran it on various datasets. The results reported in the paper look extremely clear and are reported mostly on outdoor scenes. We see that the model is able to sufficiently recover detail in low lighting conditions, making the images look as though they were taken during the daytime. 
                        </p>
                        <figure class="normal-image-container" style="display: inline-block; width: 700px; max-width: 700px;">
                            <img src="assets/enlightengan/paper-results.png" style="height: 250px; max-width: 700px; width: 700px;">
                            <figcaption>Baseline: EnlightenGAN Results</figcaption>
                        </figure>
                        <p>
                            However, these high quality results seem to only hold for these outdoor images. When testing on indoor datasets such as LOL (LOw-Light Dataset) or People's Illuminations, we see certain artifacts. 
                            Namely, there exist lots of noise in the final result and in some cases the hue over the image is green. 
                        </p>
                        <figure class="normal-image-container" style="display: inline-block; width: 325px; max-width: 325px;">
                            <img src="assets/enlightengan/exp1-noise.png" style="height: 50px; max-width: 325px; width: 325px;">
                            <figcaption>EnlightenGAN Baseline on LOL dataset (noisy)</figcaption>
                        </figure>

                        <figure class="normal-image-container" style="display: inline-block; width: 325px; max-width: 325px;">
                            <img src="assets/enlightengan/exp1-hue.png" style="height: 50px; max-width: 325px; width: 325px;">
                            <figcaption>EnlightenGAN Baseline on People Illumination dataset (hue)</figcaption>
                        </figure>
                        
                        </p>
                    </section>

                    <section>
                        <h4>2. Experiment 2</h4>
                    </section>
                 
                </section>
            </section>

            <!-- StyleGAN-NADA -->
            <section id="stylegan-nada">
                <h2>Method 2: StyleGAN-NADA</h2>
                <p>
                    Another approach we explored, was the use of <a href="https://stylegan-nada.github.io/">StyleGAN-NADA</a> for illumination recovery. The model uses 2 text prompts and the semantic knowledge of CLIP embeddings to describe a source and target domain for an image, and aims to generate a new image in the style defined by the user. Because it works in a zero-shot manner, it is flexible in generating images from domains the pretrained generators have not seen before. We can take advantage of this to observe the results with some prompt engineering in an attempt to move the low-light image to a domain with better lighting. If it doesn’t work well, we can look into manipulating the CLIP embeddings to be more familiar with the requested transition and give better results. 
                </p>
                <section id="results-two" id="subsection">
                    <h3> Experiments / Results</h3>

                    <p>Description_of_experiments_results (feel free to break up into multiple subsections)</p>
                </section>
                
            </section>
            
            <section id="control-net">
                <h2>Method 3: Control Net</h2>
                <p>
                    <a href="https://arxiv.org/abs/2302.05543">Control-Net</a> is a very recent paper that proposes a way to condition text-to-image generation on extra features in addition to just input text prompts. This allows us to exert greater control over the kind of output image that gets generated. For instance, if you want to generate an image of a “stormtrooper giving a lecture at university”, but want it follow the pose in the picture below (left), you can supply the depth-map (middle, the “extra feature”) in addition to the text prompt, to get the desired generation (right).
                </p>

                <div class="panel" style="text-align: center;">
                    <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                        <img src="assets/control_net/in.png" style="height: 200px; width: 200px;">
                        <figcaption>Input</figcaption>
                    </figure>
                    <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                        <img src="assets/control_net/depth.png" style="height: 200px; width: 200px;">
                        <figcaption>Depth</figcaption>
                    </figure>
                    <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                        <img src="assets/control_net/out.png" style="height: 200px; width: 200px;">
                        <figcaption>Output</figcaption>
                    </figure>
                </div>

                <p>
                    Since this extra feature could be anything (edge map, pose coordinates, segmentation maps, etc.), we fine-tuned the Control-Net model with an image in low illumination as the control feature, to recover the contents of the low light image, as well as experiment with prompts to see what the image might look like in daylight. For the <a href="http://yaksoy.github.io/faid/#:~:text=Processed%20photographs-,People%20(4.3GB),-People%20(1.3GB)">People's Illumination</a> dataset, here's an sample datapoint where you can see the "flash" (low light) image on the left, and the "ambient" (illumination recovered) image on the right.
                </p>
                <div class="panel" style="text-align: center;">
                    <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                        <img src="assets/control_net/peoples_in.png" style="height: 200px; width: 200px;">
                        <figcaption>Flash Image (low light)</figcaption>
                    </figure>
                    <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                        <img src="assets/control_net/peoples_out.png" style="height: 200px; width: 200px;">
                        <figcaption>Ambient Image (desired)</figcaption>
                    </figure>
                </div>

                <section id="results-three" id="subsection">
                    <h3> Experiments / Results</h3>
                    <p>
                        We attempted to training control-net on two datasets from the <a href="http://yaksoy.github.io/faid/">Flash and Ambient Illuminations Dataset</a>, namely the <a href="http://yaksoy.github.io/faid/#:~:text=Processed%20photographs-,People%20(4.3GB),-People%20(1.3GB)">People's Illumination</a> dataset, and the <a href="http://yaksoy.github.io/faid/#:~:text=Shelves%20(1.7GB)-,Plants%20(3.2GB),-Plants%20(1.1GB)">Plant's Illumination</a> dataset. Here are a few results after training for ~30,000 iterations on each dataset. 
                    </p>

                    <div style="font-family: 'Raleway'; font-size: 16px; font-style: italic; border: 1px dashed #fa7564; padding: 1px 2px; text-align: center;">
                        <p>
                            Note: We used the prompt "picture in ambient light" for all image generations.
                        </p>
                    </div>

                    <div class="panel" style="text-align: center;">
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output1/flash.png" style="height: 200px; width: 200px;">
                            <figcaption>Low light image (flash)</figcaption>
                        </figure>
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output1/ambient.png" style="height: 200px; width: 200px;">
                            <figcaption>Ambient light image</figcaption>
                        </figure>
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output1/diffusion.png" style="height: 200px; width: 200px;">
                            <figcaption>Control-Net Output</figcaption>
                        </figure>
                    </div>

                    <div class="panel" style="text-align: center;">
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output2/flash.png" style="height: 200px; width: 200px;">
                            <figcaption>Low light image (flash)</figcaption>
                        </figure>
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output2/ambient.png" style="height: 200px; width: 200px;">
                            <figcaption>Ambient light image</figcaption>
                        </figure>
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output2/diffusion.png" style="height: 200px; width: 200px;">
                            <figcaption>Control-Net Output</figcaption>
                        </figure>
                    </div>

                    <div class="panel" style="text-align: center;">
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output3/flash.png" style="height: 200px; width: 200px;">
                            <figcaption>Low light image (flash)</figcaption>
                        </figure>
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output3/ambient.png" style="height: 200px; width: 200px;">
                            <figcaption>Ambient light image</figcaption>
                        </figure>
                        <figure class="normal-image-container" style="display: inline-block; width: 200px;">
                            <img src="assets/control_net/output3/diffusion.png" style="height: 200px; width: 200px;">
                            <figcaption>Control-Net Output</figcaption>
                        </figure>
                    </div>
                    
                    <p>
                        A few points worth noting here:
                        <li type="-">
                            The synthesized image from control-net usually looks brighter (more illumination) than the ground truth image.
                        </li>
                        <li type="-">
                            For people's illumination, the identity of the person, and lower level details of the room are lost (see middle row) - eventhough the higher level semantics like the angle of the face, and shape of the background are preserved.
                        </li>
                        <li type="-">
                            The results from the plants dataset looks better - probably because it is easier to guess plant structure than the finer details of human faces.
                        </li>
                    </p>
                </section>
            </section>

            <section id="conclusion">
                <h2>Conclusion</h2>
            </section>

        </div>
        
        <nav class="section-nav">
            <ol>
                <li><a href="#introduction">Project Description</a>
                </li>
                <li><a href="#enlighten-gan">Method 1: Englighten GAN</a>
                    <ul>
                        <li class=""><a href="#results-one">- Experiments / Results</a></li>
                    </ul>
                </li>
                <li><a href="#stylegan-nada">Method 2: StyleGAN-NADA</a>
                    <ul>
                        <li class=""><a href="#results-two">- Experiments / Results</a></li>
                    </ul>
                </li>
                <li><a href="#control-net">Method 3: Control Net</a>
                    <ul>
                        <li class=""><a href="#results-three">- Experiments / Results</a></li>
                    </ul>
                </li>
                <li><a href="#conclusion">Conclusion</a></li>
            </ol>
        </nav>
    </main>
</body>
</html>
